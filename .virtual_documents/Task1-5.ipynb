


from textblob import TextBlob
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib
import nltk
import nltk
import re
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')
from collections import Counter
sns.set()





# Import txt file

with open('20th_Century_article_Wiki.txt', 'r', errors='ignore') as file: 
   data = file.read().replace( '\n', ' ')


# tokenize the text into sentences





# Word tokenization

from nltk.tokenize import word_tokenize      
tokenized_word = word_tokenize(data)           #tokenize text into single words using word_tokenize() function:
print(tokenized_word) 


# Look at frequency distribution of most common words


from nltk.probability import FreqDist
dist_words = FreqDist(tokenized_word)
print(dist_words)


# Create a frequency distribution of 10 most common words


top10 = dist_words.most_common(10)


# Frequency Distribution Plot

plt.figure(figsize=(8, 3))
dist_words.plot(10,cumulative = False)
plt.show()


#Creating bar plot of 10 most common words

#plt.figure(figsize = (10, 5))
#with sns.dark_palette("xkcd:blue", 10):
#      sns.barplot(x = "Words", y = "Counts",
#    saturation = 0.9, data = top10).set_title("Key Events of 20th Century - top 10 words used")





# Defining stopwords

from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
print(stop_words)


# Removing stop words
# stop words are simple words that convey no info
# Check if words in text are in stop word list (stop_words) of NLTK library

filtered_words = [] # creates an empty list
for word in tokenized_word:
      if word not in stop_words:
           filtered_words.append(word)


# Create a 2nd frequency distribution with stop words removed
# Create a new FreqDist of filteredâ€“words

dist_words_filter = FreqDist(filtered_words)
print (dist_words_filter)


# Create bar plot of 10 common words with stop words removed
# Bar Plot of Frequency Distribution of filtered-words

plt.figure(figsize=(8, 3))
dist_words_filter.plot(10,cumulative = False)
plt.show()


# Remove common punctuation marks
# Substitute all punctuation marks with a space

sans_punc = re.sub("[^a-zA-Z]",  # Search for all non-letters
                        " ",        # Replace all non-letters with spaces
                        str(filtered_words))


# Tokenize text into single words without punctuation marks
# Word tokenization

tokenized_word_2 = word_tokenize(sans_punc)
print (tokenized_word_2)


# Create a 3rd frequency distribution with punctuation marks removed from single words
# Create a new FreqDist

dist_words_filter_2 = FreqDist(tokenized_word_2)


# Create bar plot of single words with punctuation marks removed
# Bar Plot of Frequency Distribution of filtered-words

plt.figure(figsize=(8, 3))
dist_words_filter_2.plot(30,cumulative = False)
plt.show()


# Remove a few more words and letters from list of single words w/o punctuation marks (tokenized_word_2)


new_stopwords = ["And", "Then", 'n', 't', 's', 'The']

filtered = []
for word in tokenized_word_2:
     if word not in new_stopwords:
        filtered.append(word)





    # Create TextBlob object - 1st thing to do when creating tags.


%%time
text = TextBlob(str(filtered))


text


     # Use tags fcn - Next step to using POS (Part of Speech) tags - 


tags_list = text.tags


tags_list





    # Group the words by its part of speech, and narrow it down to largest 10 groups on list


df_text = pd.DataFrame(tags_list)
df_text.columns = [ '# of Words', 'Word type']
df_t=df_text.groupby('Word type').count().reset_index()
top10=df_t.nlargest(10, '# of Words')





    # Creating bar plot


plt.figure(figsize = (10, 5))
with sns.dark_palette("xkcd:blue", 10):
      sns.barplot(x = "# of Words", y = "Word type",
     saturation = 0.9, data = top10).set_title("Key Events of 20th Century - top 10 word types used")








# Filter out Nouns - singular nouns (NN), pluralized nouns (NNS), and proper nouns (NNP)


df = df_text[(df_text['Word type'] == "NN") | (df_text['Word type'] == "NNS") | (df_text['Word type'] == "NNS")]
df.columns = ['Noun', 'Occurrences']
x = df.groupby('Noun').count().reset_index()
y = x.sort_values(by = ['Occurrences'], ascending=False)
top15 = y.nlargest(15, 'Occurrences')


top15


# Create bar plot of top 15 used nouns


plt.figure(figsize=(10, 5))
with sns.dark_palette("xkcd:blue", 15):
      sns.barplot(x='Noun', y='Occurrences',
      saturation=0.9, data = top15).set_title("Key Events of 20th century - top 15 used nouns")





# Filter out Verbs - base form verbs - take (VB); past tense verbs - took (VBD); gerund/present participle verb - taking (VBG)
#                    past participle verbs - taken (VBN); sing. present, non-3d verbs- take (VBP)
#                    3rd person sing. present verbs - takes (VBZ)


df = df_text[(df_text['Word type'] == "VB") | (df_text['Word type'] == "VBD") | (df_text['Word type'] == "VBG")| (df_text['Word type'] == "VBN") | (df_text['Word type'] == "VBP")| (df_text['Word type'] == "VBZ")]
df.columns = ['Verb', 'Occurrences']
x = df.groupby('Verb').count().reset_index()
y = x.sort_values(by = ['Occurrences'], ascending=False)
top15 = y.nlargest(15, 'Occurrences')


# Create bar plot of top 15 used verbs


plt.figure(figsize=(10, 5))
with sns.dark_palette("xkcd:blue", 15):
      sns.barplot(x='Verb', y='Occurrences',
      saturation=0.9, data = top15).set_title("Key Events of 20th century - top 15 used verbs")





# Filter out Adjectives - 'big' adjectives (JJ), comparative 'bigger' adjectivess (JJR), and superlative 'biggest' adjectives (JJS)


df = df_text[(df_text['Word type'] == "JJ") | (df_text['Word type'] == "JJR") | (df_text['Word type'] == "JJS")]
df.columns = ['Adjective', 'Occurrences']
x = df.groupby('Adjective').count().reset_index()
y = x.sort_values(by = ['Occurrences'], ascending=False)
top15 = y.nlargest(15, 'Occurrences')


# Create bar plot of top 15 used adjectives


plt.figure(figsize=(10, 5))
with sns.dark_palette("xkcd:blue", 15):
      sns.barplot(x='Adjective', y='Occurrences',
      saturation=0.9, data = top15).set_title("Key Events of 20th century - top 15 used verbs")








# 1st step -extract all of the words from the tokenized data


listToStr = ' '.join([str(elem) for elem in filtered])

print(listToStr)


# Create a count for the countries
# Use the regular expression W+ inside a Counter() function to store results in a dictionary so that you can see how frequently each of the ountries appear

all_counts = Counter(re.sub(r'\W+', ' ', data).split())


all_counts


countr = pd.read_csv("adj_countries_list_20th_century_1.5.csv", index_col = 0)


countr.head()


#countr.drop(columns = ['Unnamed:2'], axis=1, inplace=True)


#Replace names with aliases


countr['country_name'] = countr['country_name'].replace('Antigua and Barbuda','Antigua_and_Barbuda')

countr['country_name'] = countr['country_name'].replace('Bahamas, The','Bahamas')

countr['country_name'] = countr['country_name'].replace('Bosnia and Herzegovina','Bosnia')
countr['country_name'] = countr['country_name'].replace('Burkina Faso','Burkina_Faso')


countr['country_name'] = countr['country_name'].replace('Cape Verde','Cape_Verde')
countr['country_name'] = countr['country_name'].replace('Central African Republic','CAR')
countr['country_name'] = countr['country_name'].replace("China, People's Republic of","China")
countr['country_name'] = countr['country_name'].replace('Congo, Democratic Republic of the','DRC')
countr['country_name'] = countr['country_name'].replace('Congo, Republic of the','Congo')
countr['country_name'] = countr['country_name'].replace('Costa Rica','Costa_Rica')
countr['country_name'] = countr['country_name'].replace('Czech Republic','Czechia')

countr['country_name'] = countr['country_name'].replace('Dominican Republic','DR')
countr['country_name'] = countr['country_name'].replace('East Timor','East_Timor')
countr['country_name'] = countr['country_name'].replace('El Salvador','El_Salvador')

countr['country_name'] = countr['country_name'].replace('Equatorial Guinea','Guinea')

countr['country_name'] = countr['country_name'].replace('Gambia, The','Gambia')

countr['country_name'] = countr['country_name'].replace('Ivory Coast','Ivory_Coast')
countr['country_name'] = countr['country_name'].replace('Korea, North', 'North_Korea')
countr['country_name'] = countr['country_name'].replace('Korea, South','South_Korea')

countr['country_name'] = countr['country_name'].replace('Marshall Islands','Marshall_Islands')
countr['country_name'] = countr['country_name'].replace('Micronesia, Federated States of','Micronesia')

countr['country_name'] = countr['country_name'].replace('New Zealand','New_Zealand')
countr['country_name'] = countr['country_name'].replace('North Macedonia','North_Macedonia')
countr['country_name'] = countr['country_name'].replace('Papua New Guinea','Papua_New_Guinea')

countr['country_name'] = countr['country_name'].replace('Saint Kitts and Nevis','Saint_Kitts_and_Nevis')
countr['country_name'] = countr['country_name'].replace('Saint Lucia','Saint_Lucia')
countr['country_name'] = countr['country_name'].replace('Saint Vincent and the Grenadines','Saint_Vincent_and_Grenadines')
countr['country_name'] = countr['country_name'].replace('San Marino','San_Marino')
countr['country_name'] = countr['country_name'].replace('Sao and Principe','Sao_and_Principe')
countr['country_name'] = countr['country_name'].replace('Saudi Arabia','Saudi_Arabia')
countr['country_name'] = countr['country_name'].replace('Sierre Leone','Sierre_Leone')
countr['country_name'] = countr['country_name'].replace('Solomon Islands','Solomon_Islands')
countr['country_name'] = countr['country_name'].replace('South Africa','South_Africa')
countr['country_name'] = countr['country_name'].replace('South Sudan','South_Sudan')
countr['country_name'] = countr['country_name'].replace('Sri Lanka','Sri_Lanka')
countr['country_name'] = countr['country_name'].replace('Trinidad and Tobago','Trinidad_and_Tobago')


countr['country_name'] = countr['country_name'].replace('United Arab Emirates','UAE')
countr['country_name'] = countr['country_name'].replace('United Kingdom','UK')
countr['country_name'] = countr['country_name'].replace('United States','US')
countr['country_name'] = countr['country_name'].replace('Vatican City (Holy See)','Vatican_City')

countr['country_name'] = countr['country_name'].replace('Cook Islands','Cook_Islands')
countr['country_name'] = countr['country_name'].replace("Donetsk People's Republic","DPR")
countr['country_name'] = countr['country_name'].replace("Luhansk People's Republic","LPR")

countr['country_name'] = countr['country_name'].replace('Northern Cyprus','Northern_Cyprus')
countr['country_name'] = countr['country_name'].replace('Sahrawi Arab Democratic Republic','Sahrawi_Republic')
countr['country_name'] = countr['country_name'].replace('South Ossetta','South_Ossetta')



#Remove country names that begin with "The"


countr['country_alias'] = countr['country_name'].apply(lambda x: x.rsplit(' ',1)[-1])


countr_list = countr['country_alias'].to_list()


dict_of_counts = {d : all_counts[d] for d in countr_list}


dict_of_counts


#Search for the names from the list in the dictionary

dct = {v:[k] for v,k in dict_of_counts.items()}
df = pd.DataFrame(dct)


df


df = df.transpose().reset_index()


df.dtypes


df


df.rename(columns = {"index":"country_name", 0:"Times mentioned"}, inplace = True)


df


df.shape





plt.figure(figsize=(10, 5))
with sns.dark_palette("#79C", 27):
    sns.barplot(x = "Times mentioned", y = "Country name",
    saturation=0.9, data = df.sort_values("Times mentioned", ascending = False)).set_title("Key Events of 20th Century - most frequently mentioned countries")



